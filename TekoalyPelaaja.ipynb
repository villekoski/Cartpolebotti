{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0aac0-0c84-4f98-9b71-570b961cfe88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43724206-df1a-4454-a4c8-e324e50ea30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mallin_rakennus(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation ='linear'))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mallin_rakennus(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentin_rakennus(malli, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=20, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ": 219, duration: 0.686s, episode steps: 200, steps per second: 291, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.205734, mae: 42.905869, mean_q: 86.499802\n",
      " 33696/50000: episode: 220, duration: 0.699s, episode steps: 199, steps per second: 285, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.139181, mae: 42.739338, mean_q: 86.143196\n",
      " 33896/50000: episode: 221, duration: 0.688s, episode steps: 200, steps per second: 291, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.156749, mae: 43.066219, mean_q: 86.811546\n",
      " 34096/50000: episode: 222, duration: 0.690s, episode steps: 200, steps per second: 290, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.559877, mae: 42.936813, mean_q: 86.508011\n",
      " 34296/50000: episode: 223, duration: 0.690s, episode steps: 200, steps per second: 290, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.637979, mae: 42.969452, mean_q: 86.506950\n",
      " 34496/50000: episode: 224, duration: 0.691s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.997612, mae: 43.429409, mean_q: 87.314880\n",
      " 34696/50000: episode: 225, duration: 0.689s, episode steps: 200, steps per second: 290, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 7.393538, mae: 42.845558, mean_q: 86.248474\n",
      " 34896/50000: episode: 226, duration: 0.690s, episode steps: 200, steps per second: 290, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 8.121860, mae: 43.099728, mean_q: 86.813942\n",
      " 35096/50000: episode: 227, duration: 0.698s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.231755, mae: 42.718212, mean_q: 85.999779\n",
      " 35296/50000: episode: 228, duration: 0.689s, episode steps: 200, steps per second: 290, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.226170, mae: 42.983799, mean_q: 86.559280\n",
      " 35496/50000: episode: 229, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.561654, mae: 42.922501, mean_q: 86.403610\n",
      " 35696/50000: episode: 230, duration: 0.712s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.600924, mae: 42.904716, mean_q: 86.509483\n",
      " 35896/50000: episode: 231, duration: 0.710s, episode steps: 200, steps per second: 282, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 10.862473, mae: 42.892311, mean_q: 86.195564\n",
      " 36067/50000: episode: 232, duration: 0.593s, episode steps: 171, steps per second: 288, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.487954, mae: 42.678829, mean_q: 85.966919\n",
      " 36241/50000: episode: 233, duration: 0.623s, episode steps: 174, steps per second: 279, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 6.324996, mae: 42.908264, mean_q: 86.458824\n",
      " 36441/50000: episode: 234, duration: 0.725s, episode steps: 200, steps per second: 276, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.112782, mae: 42.465710, mean_q: 85.529610\n",
      " 36640/50000: episode: 235, duration: 0.718s, episode steps: 199, steps per second: 277, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 7.849311, mae: 42.498165, mean_q: 85.549072\n",
      " 36840/50000: episode: 236, duration: 0.713s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.165731, mae: 42.580311, mean_q: 85.827538\n",
      " 37040/50000: episode: 237, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.207440, mae: 42.124687, mean_q: 84.877060\n",
      " 37231/50000: episode: 238, duration: 0.674s, episode steps: 191, steps per second: 283, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.654757, mae: 42.443558, mean_q: 85.602997\n",
      " 37431/50000: episode: 239, duration: 0.715s, episode steps: 200, steps per second: 280, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 8.635816, mae: 42.395775, mean_q: 85.312477\n",
      " 37631/50000: episode: 240, duration: 0.704s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 9.045507, mae: 42.233089, mean_q: 84.995941\n",
      " 37831/50000: episode: 241, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.244929, mae: 42.004906, mean_q: 84.613724\n",
      " 38031/50000: episode: 242, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.441913, mae: 42.203060, mean_q: 84.902039\n",
      " 38230/50000: episode: 243, duration: 0.694s, episode steps: 199, steps per second: 287, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.457554, mae: 41.830162, mean_q: 84.297585\n",
      " 38430/50000: episode: 244, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 4.009925, mae: 41.633839, mean_q: 84.036896\n",
      " 38630/50000: episode: 245, duration: 0.704s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.830271, mae: 42.002174, mean_q: 84.574036\n",
      " 38830/50000: episode: 246, duration: 0.704s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.607203, mae: 41.805328, mean_q: 84.194687\n",
      " 39030/50000: episode: 247, duration: 0.711s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.418412, mae: 41.833916, mean_q: 84.397522\n",
      " 39218/50000: episode: 248, duration: 0.675s, episode steps: 188, steps per second: 279, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 4.692185, mae: 41.774620, mean_q: 84.294571\n",
      " 39418/50000: episode: 249, duration: 0.711s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.192486, mae: 41.816830, mean_q: 84.316872\n",
      " 39618/50000: episode: 250, duration: 0.702s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.613307, mae: 41.743633, mean_q: 84.119255\n",
      " 39818/50000: episode: 251, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.576284, mae: 42.009533, mean_q: 84.654846\n",
      " 40018/50000: episode: 252, duration: 0.698s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.352098, mae: 41.808224, mean_q: 84.202225\n",
      " 40218/50000: episode: 253, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.615609, mae: 42.008350, mean_q: 84.674408\n",
      " 40418/50000: episode: 254, duration: 0.699s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.764453, mae: 41.694016, mean_q: 84.143867\n",
      " 40618/50000: episode: 255, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.289851, mae: 41.942345, mean_q: 84.356018\n",
      " 40818/50000: episode: 256, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.054988, mae: 41.804447, mean_q: 84.211273\n",
      " 41018/50000: episode: 257, duration: 0.703s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.156171, mae: 41.809216, mean_q: 84.171349\n",
      " 41210/50000: episode: 258, duration: 0.669s, episode steps: 192, steps per second: 287, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 8.006398, mae: 41.574100, mean_q: 83.677773\n",
      " 41410/50000: episode: 259, duration: 0.698s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.354855, mae: 41.735176, mean_q: 84.024040\n",
      " 41610/50000: episode: 260, duration: 0.702s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.251848, mae: 41.214832, mean_q: 82.971184\n",
      " 41810/50000: episode: 261, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 8.699071, mae: 41.353397, mean_q: 83.201897\n",
      " 42008/50000: episode: 262, duration: 0.690s, episode steps: 198, steps per second: 287, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.704327, mae: 41.285496, mean_q: 83.215462\n",
      " 42208/50000: episode: 263, duration: 0.701s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.196443, mae: 41.510811, mean_q: 83.632660\n",
      " 42408/50000: episode: 264, duration: 0.693s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.547581, mae: 41.145897, mean_q: 82.955803\n",
      " 42608/50000: episode: 265, duration: 0.703s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.169570, mae: 40.959633, mean_q: 82.555283\n",
      " 42808/50000: episode: 266, duration: 0.730s, episode steps: 200, steps per second: 274, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 6.942739, mae: 41.588715, mean_q: 83.815605\n",
      " 43008/50000: episode: 267, duration: 0.719s, episode steps: 200, steps per second: 278, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.854532, mae: 41.474491, mean_q: 83.530449\n",
      " 43208/50000: episode: 268, duration: 0.713s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 5.073657, mae: 41.387852, mean_q: 83.320694\n",
      " 43408/50000: episode: 269, duration: 0.717s, episode steps: 200, steps per second: 279, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.926415, mae: 41.752968, mean_q: 83.978798\n",
      " 43588/50000: episode: 270, duration: 0.645s, episode steps: 180, steps per second: 279, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 7.959092, mae: 41.347706, mean_q: 83.097519\n",
      " 43788/50000: episode: 271, duration: 0.705s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.497110, mae: 41.155430, mean_q: 82.706932\n",
      " 43988/50000: episode: 272, duration: 0.699s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.305200, mae: 41.058632, mean_q: 82.602356\n",
      " 44188/50000: episode: 273, duration: 0.726s, episode steps: 200, steps per second: 275, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.713412, mae: 41.199818, mean_q: 82.855232\n",
      " 44388/50000: episode: 274, duration: 0.735s, episode steps: 200, steps per second: 272, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.599388, mae: 41.164894, mean_q: 82.770294\n",
      " 44588/50000: episode: 275, duration: 0.709s, episode steps: 200, steps per second: 282, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.469129, mae: 41.087181, mean_q: 82.673981\n",
      " 44788/50000: episode: 276, duration: 0.721s, episode steps: 200, steps per second: 277, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.619011, mae: 40.954655, mean_q: 82.443558\n",
      " 44988/50000: episode: 277, duration: 0.720s, episode steps: 200, steps per second: 278, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.141746, mae: 41.262760, mean_q: 82.994942\n",
      " 45188/50000: episode: 278, duration: 0.732s, episode steps: 200, steps per second: 273, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.078681, mae: 41.076679, mean_q: 82.701897\n",
      " 45388/50000: episode: 279, duration: 0.735s, episode steps: 200, steps per second: 272, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.098809, mae: 41.114887, mean_q: 82.640877\n",
      " 45588/50000: episode: 280, duration: 0.726s, episode steps: 200, steps per second: 276, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.724928, mae: 40.864975, mean_q: 82.318985\n",
      " 45788/50000: episode: 281, duration: 0.709s, episode steps: 200, steps per second: 282, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.688152, mae: 40.663517, mean_q: 81.710693\n",
      " 45988/50000: episode: 282, duration: 0.717s, episode steps: 200, steps per second: 279, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.435623, mae: 40.777695, mean_q: 81.984261\n",
      " 46188/50000: episode: 283, duration: 0.701s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.348756, mae: 40.572102, mean_q: 81.624626\n",
      " 46388/50000: episode: 284, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.040838, mae: 40.999889, mean_q: 82.600906\n",
      " 46556/50000: episode: 285, duration: 0.583s, episode steps: 168, steps per second: 288, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.500794, mae: 40.708313, mean_q: 81.943153\n",
      " 46756/50000: episode: 286, duration: 0.700s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 8.036079, mae: 40.832214, mean_q: 82.104881\n",
      " 46956/50000: episode: 287, duration: 0.715s, episode steps: 200, steps per second: 280, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.597414, mae: 40.623611, mean_q: 81.792213\n",
      " 47156/50000: episode: 288, duration: 0.696s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.230139, mae: 40.831558, mean_q: 82.036018\n",
      " 47356/50000: episode: 289, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 9.099304, mae: 41.332050, mean_q: 82.891800\n",
      " 47556/50000: episode: 290, duration: 0.715s, episode steps: 200, steps per second: 280, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.876099, mae: 40.697517, mean_q: 81.825615\n",
      " 47754/50000: episode: 291, duration: 0.691s, episode steps: 198, steps per second: 287, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 6.683531, mae: 40.163109, mean_q: 80.811478\n",
      " 47954/50000: episode: 292, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.793183, mae: 40.379562, mean_q: 81.110687\n",
      " 48154/50000: episode: 293, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 10.404862, mae: 40.273239, mean_q: 80.811485\n",
      " 48354/50000: episode: 294, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 7.301777, mae: 40.524006, mean_q: 81.426338\n",
      " 48554/50000: episode: 295, duration: 0.689s, episode steps: 200, steps per second: 290, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.097593, mae: 40.321827, mean_q: 81.016136\n",
      " 48754/50000: episode: 296, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.503789, mae: 40.539574, mean_q: 81.526894\n",
      " 48925/50000: episode: 297, duration: 0.590s, episode steps: 171, steps per second: 290, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.705673, mae: 40.425503, mean_q: 81.266335\n",
      " 49125/50000: episode: 298, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.959232, mae: 40.194080, mean_q: 80.628326\n",
      " 49325/50000: episode: 299, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.567679, mae: 40.386898, mean_q: 81.116791\n",
      " 49525/50000: episode: 300, duration: 0.691s, episode steps: 200, steps per second: 290, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.826856, mae: 40.713512, mean_q: 81.639496\n",
      " 49725/50000: episode: 301, duration: 0.705s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.722595, mae: 40.335007, mean_q: 81.007736\n",
      " 49925/50000: episode: 302, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 9.698640, mae: 40.436329, mean_q: 81.141060\n",
      "done, took 171.708 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x217e42e87b8>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dqn = agentin_rakennus(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34199f8-0247-46c0-81fd-d629474c55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = mallin_rakennus(states, actions)\n",
    "dqn = agentin_rakennus(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079fd1d0-d42b-4196-b16f-c57135b7b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2845a9e-0de3-4665-baa9-70a7f4dcab21",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing for 5 episodes ...\n",
      "WARNING:tensorflow:From C:\\Users\\ville\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "testi = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python370jvsc74a57bd0b8e8889d2ed9c1869965e3528dd57cb3e495eb84a1a257b1d43dd6ffe79cc3a2",
   "display_name": "Python 3.7.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}